{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff26174-77ef-492a-b477-3483f5964f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2326b2b-ed4f-4bcb-8aad-77a17367acda",
   "metadata": {},
   "source": [
    "# <font color=\"darkgray\">Classical Reservoir Computing:</font><br>Mathematical Foundations\n",
    "\n",
    "<table>\n",
    "    <tr><td><strong>Aim:</strong></td>\n",
    "        <td>To formulate a classical reservoir computing model, so that it could be implemented \n",
    "            in <strong>Python</strong>.</tr>\n",
    "    <tr><td><strong>Author:</strong></td>\n",
    "        <td>Jacob L. Cybulski (<a href=\"https://jacobcybulski.com/\" target=\"_blank\">website</a>),\n",
    "            <em>Enquanted</em></td></tr>\n",
    "    <tr><td><strong>Release:</strong></td>\n",
    "        <td>April 2025</td></tr>\n",
    "    <tr><td><strong>References:</strong></td>\n",
    "        <td>Unfortunately, there is no single simple formulation of a classical reservoir. \n",
    "            Numerous papers have been checked for some good insights, including:\n",
    "            <ul>\n",
    "            <li><a href = \"https://doi.org/10.1103/PhysRevApplied.14.024065\" target=\"_blank\">\n",
    "                Chen, J., Nurdin, H.I., Yamamoto, N., 2020. Temporal Information Processing on Noisy Quantum Computers. Phys. Rev. Applied 14, 024065.\n",
    "</a></li>\n",
    "            <li><a href = \"https://doi.org/10.1088/2634-4386/ac7db7\" target=\"_blank\">Cucchi, M., Abreu, S., Ciccone, G., Brunner, D., Kleemann, H., 2022. Hands-on reservoir computing: a tutorial for practical implementation. Neuromorph. Comput. Eng. 2, 032002. \n",
    "</a></li>\n",
    "            <li><a href = \"https://doi.org/10.1038/s41467-021-25801-2\" target=\"_blank\">Gauthier, D.J., Bollt, E., Griffith, A., Barbosa, W.A.S., 2021. Next generation reservoir computing. Nat Commun 12, 5564.\n",
    "</a></li>\n",
    "        </ul>\n",
    "        And many others...</td>\n",
    "    </tr>\n",
    "    <tr><td><strong>License:</strong></td>\n",
    "        <td>This project is licensed under the\n",
    "            <a href=\"./LICENSE\" target=\"_blank\">GNU General Public License V3</a></td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248410a9-c530-4019-98c2-ea1bb2e1e2da",
   "metadata": {},
   "source": [
    "### Reservoir computing (RC)\n",
    "RC is a machine learning model that integrates elements of a family of temporal neural networks, such as recurrent neural networks, liquid-state machines and echo-state networks.\n",
    "While RCs can be applied to all types of data, they are especially useful for the processing of temporal data. \n",
    "The reservoir computing architecture centers around a large sparse neural network of randomly initialised and fixed weights, called a (_**reservoir**_), \n",
    "which is responsible for transforming input into a higher-dimensional space.\n",
    "In high-dimensional space, the transformed input can then be used to train a simple linear model (_**readout layer**_) \n",
    "to separate (e.g. for the purpose of _**classification**_) the reservoir states with relative ease.\n",
    "The _**reservoir dynamics**_ can be specified by a set of differential equations (_**update rules**_)\n",
    "responsible for describing changes to the reservoir state over time.\n",
    "\n",
    "Typical reservoir computing applications include: time-series forecasting, speech recognition and video analysis, control of robots or autonomous vehicles, as well as, predicting weather patterns and stock markets.\n",
    "It is also worth noting that RC concepts also apply to physical systems that are continuous in space and time.\n",
    "\n",
    "This is Jacob's take on the RC working!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26618a7f-c1b3-4d4c-89f1-9dd070ff6d76",
   "metadata": {},
   "source": [
    "### 1. Reservoir dynamics\n",
    "\n",
    "The reservoir needs to be designed to have the following two properties, e.g.\n",
    "- **Echo state property (ESP)**: The reservoir should be able to retain (or \"echo\") past information for a short period of time.\n",
    "- **Fading memory**: The influence of the past inputs on the reservoir state should fade over time.\n",
    "\n",
    "Therefore the reservoir dynamics can be described as follows:\n",
    "$$\n",
    "     \\mathbf{r}(t+1) = f(\\mathbf{W}_{\\text{in}} \\mathbf{u}(t) + \\mathbf{W}_{\\text{res}} \\mathbf{r}(t))\n",
    "$$\n",
    "where:\n",
    "- $\\mathbf{r}(t)$ is the reservoir state at time $t$,\n",
    "- $\\mathbf{u}(t)$ is the input at time $t$,\n",
    "- $\\mathbf{W}_{\\text{in}}$ is a matrix transforming input to the reservoir weight matrix  (fixed, small and random),\n",
    "- $\\mathbf{W}_{\\text{res}}$ is the reservoir weight matrix (sparse, fixed and random),\n",
    "- $f$ is a nonlinear activation function (e.g., $\\tanh$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399f2a63-d207-47b5-abcc-9667deaf0767",
   "metadata": {},
   "source": [
    "### 2. Reservoir Weight Matrix\n",
    "\n",
    "The reservoir weight matrix $\\mathbf{W}_{\\text{res}}$ should be sparse and randomly initialized.\n",
    "To ensure that $\\mathbf{W}_{\\text{res}}$ does not have very large or very small weights, which may prevent the short-term memory stability (ESP) or fading long-term memory, we scale the reservoir weights by the **spectral radius** $\\rho(\\mathbf{W}_{\\text{res}})$ (the largest absolute eigenvalue):\n",
    "$$\n",
    "       \\mathbf{W}_{\\text{res}} = \\frac{\\alpha}{\\rho(\\mathbf{W}_{\\text{res}})} \\mathbf{W}_{\\text{res}}\n",
    "$$\n",
    "       where $\\alpha$ is a scaling factor (typically $0 < \\alpha < 1$, e.g., 0.9)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80d52ab-3b1b-42b0-933b-33a7ab371f23",
   "metadata": {},
   "source": [
    "### 3. Readout layer\n",
    "\n",
    "The readout layer is a simple (usually) linear model that maps the reservoir's high-dimensional states to the target output, which at time $t$ is:\n",
    "$$\\mathbf{y}(t) = \\mathbf{W}_{\\text{out}} \\mathbf{r}(t)$$\n",
    "where:\n",
    " - $\\mathbf{y}(t)$ is the output,\n",
    " - $\\mathbf{W}_{\\text{out}}$ is the output weight matrix (the only trainable part of the system),\n",
    " - $\\mathbf{r}(t)$ is the reservoir state at time $t$.\n",
    "\n",
    "The goal is to find the optimal $\\mathbf{W}_{\\text{out}}$ such that the output $\\mathbf{y}(t)$ matches the target as closely as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea9ec16-b7d3-4653-b309-a641bbe7fdb1",
   "metadata": {},
   "source": [
    "### 4. Training the readout layer\n",
    "\n",
    "Training involves collecting the reservoir states $\\mathbf{r}(t)$ over time and using them to solve for $\\mathbf{W}_{\\text{out}}$.\n",
    "\n",
    "Let us define $\\mathbf{R}$ and $\\mathbf{Y}$ as follows:\n",
    "   - Let $\\mathbf{R} \\in \\mathbb{R}^{N \\times T}$ be the matrix of reservoir states (where $ N $ is the number of reservoir \"neurons\" and $ T $ is the number of time steps).\n",
    "   - Let $\\mathbf{Y} \\in \\mathbb{R}^{d \\times T}$ be the matrix of target outputs (where $ d $ is the output dimension).\n",
    "\n",
    "\n",
    "The process involves the following steps:\n",
    "1. **Drive the Reservoir**: Feed the input data $\\mathbf{u}(t)$ into the reservoir and collect the corresponding reservoir states $\\mathbf{r}(t)$ over time.\n",
    "1. **Collect Data**: Stack the reservoir states into a matrix $\\mathbf{R}$ and the target outputs into a matrix $\\mathbf{Y}$.\n",
    "1. **Solve for $\\mathbf{W}_{\\text{out}}$** by using _**ridge regression**_ (see step 7 for explanation):\n",
    "\n",
    "$$\\mathbf{W}_{\\text{out}} = \\mathbf{Y} \\mathbf{R}^T (\\mathbf{R} \\mathbf{R}^T + \\lambda \\mathbf{I})^{-1}$$\n",
    "\n",
    "where $\\lambda$ is a regularization parameter (used in ridge regression to prevent overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d093413d-3b0b-46d6-b19d-43f84db3d5da",
   "metadata": {},
   "source": [
    "### 5. Leaky integration\n",
    "We can optionally introduce a leakage rate $\\gamma$ (e.g., 0.1 to 0.5) to control the speed of the reservoir dynamics. In this case, the state update equation becomes:\n",
    "$$\n",
    "     \\mathbf{r}(t+1) = (1-\\gamma) \\mathbf{r}(t) + \\gamma f(\\mathbf{W}_{\\text{in}} \\mathbf{u}(t) + \\mathbf{W}_{\\text{res}} \\mathbf{r}(t))\n",
    "$$\n",
    "     This helps balance fast and slow dynamics in the reservoir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f7dda2-183e-4235-8e24-8da522fa56c6",
   "metadata": {},
   "source": [
    "### 6. Test the model\n",
    "\n",
    "For each input sequence in the test set, \n",
    "it is possible to use the trained readout model to predict the next value. \n",
    "This can be achieved by the following steps:\n",
    "\n",
    "1. Update the reservoir state $\\mathbf{r}(t)$ (as in training).\n",
    "1. Compute the predicted output:\n",
    "    $$\n",
    "       \\hat{y}(t) = \\mathbf{W}_{\\text{out}} \\mathbf{r}(t)\n",
    "    $$\n",
    "1. Compare the predicted output $\\hat{y}(t)$ to the true value $y(t)$, and possibly\n",
    "measure the prediction performance using metrics such as **Mean Squared Error (MSE)**:\n",
    "    $$\n",
    "       \\text{MSE} = \\frac{1}{N} \\sum_{t=1}^N (y(t) - \\hat{y}(t))^2\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933155bc-32d4-414b-adb0-a73df43d859e",
   "metadata": {},
   "source": [
    "### 7. Why ridge regression is a solution for $W_{out}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaa98f6-923c-4797-bbe9-ff0a5a726556",
   "metadata": {},
   "source": [
    "Our objective is to:\n",
    "   - Find $ \\mathbf{W}_{\\text{out}} \\in \\mathbb{R}^{d \\times N} $ that minimizes the **mean squared error (MSE)** with **L2 regularization** (ridge penalty):\n",
    "$$\n",
    "     \\mathcal{L}(\\mathbf{W}_{\\text{out}}) = \\|\\mathbf{Y} - \\mathbf{W}_{\\text{out}} \\mathbf{R}\\|_F^2 + \\lambda \\|\\mathbf{W}_{\\text{out}}\\|_F^2,\n",
    "$$\n",
    "     where $ \\lambda \\geq 0 $ is the regularization coefficient, and $ \\|\\cdot\\|_F $ is the Frobenius norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72b0150-95de-49b4-8318-115d2ef65828",
   "metadata": {},
   "source": [
    "Let us use the ***Frobenius norm*** as a loss function. Its advantage in machine learning is that it measures squared error between expected and predicted matrix operation, it is a generalisation of Euclidean (L2) norm, it is differentiable and convex, and can be regularised.<br>\n",
    "It is defined as \n",
    "$$ \n",
    "     \\|\\mathbf{A}\\|_F^2 = \\text{tr}(\\mathbf{A}^\\top \\mathbf{A}) \n",
    "$$\n",
    "     So now:\n",
    "     \n",
    "$$\n",
    "   \\mathcal{L}(\\mathbf{W}_{\\text{out}}) = \\text{tr}\\left[ (\\mathbf{Y} - \\mathbf{W}_{\\text{out}} \\mathbf{R})^\\top (\\mathbf{Y} - \\mathbf{W}_{\\text{out}} \\mathbf{R}) \\right] + \\lambda \\, \\text{tr}(\\mathbf{W}_{\\text{out}}^\\top \\mathbf{W}_{\\text{out}}).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84aa421-e07b-425e-8946-81b437b667af",
   "metadata": {},
   "source": [
    "Let us take the gradient w.r.t. $ \\mathbf{W}_{\\text{out}} $:\n",
    "$$\n",
    "   \\nabla_{\\mathbf{W}_{\\text{out}}} \\mathcal{L} = -2 (\\mathbf{Y} - \\mathbf{W}_{\\text{out}} \\mathbf{R}) \\mathbf{R}^\\top + 2 \\lambda \\mathbf{W}_{\\text{out}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a087e51-d179-477a-b73c-f5431ddd27b2",
   "metadata": {},
   "source": [
    "To identify the minimum we find the zero gradient:\n",
    "$$\n",
    "   -2 (\\mathbf{Y} - \\mathbf{W}_{\\text{out}} \\mathbf{R}) \\mathbf{R}^\\top + 2 \\lambda \\mathbf{W}_{\\text{out}} = 0.\n",
    "$$\n",
    "    which can be simplified as:\n",
    "$$\n",
    "   \\mathbf{Y} \\mathbf{R}^\\top = \\mathbf{W}_{\\text{out}} (\\mathbf{R} \\mathbf{R}^\\top + \\lambda \\mathbf{I}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9682c98-87a9-46fb-a102-f230d3ec755f",
   "metadata": {},
   "source": [
    "When $ \\lambda > 0 $, we can now solve for $ \\mathbf{W}_{\\text{out}} $, assuming $ (\\mathbf{R} \\mathbf{R}^\\top + \\lambda \\mathbf{I}) $ is invertible:\n",
    "$$\n",
    "   \\mathbf{W}_{\\text{out}} = \\mathbf{Y} \\mathbf{R}^\\top (\\mathbf{R} \\mathbf{R}^\\top + \\lambda \\mathbf{I})^{-1}.\n",
    "$$\n",
    "\n",
    "   This is the **ridge regression solution** for the output weights in reservoir computing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc2c91e-10f6-42a8-9e47-4b3bbd0500b7",
   "metadata": {},
   "source": [
    "When $ \\lambda = 0 $ (no regularisation), we are dealing with **ordinary least squares**, in which case the solution reduces to pseudoinverse:\n",
    "$$\n",
    "\\mathbf{W}_{\\text{out}} = \\mathbf{Y} \\mathbf{R}^\\top (\\mathbf{R} \\mathbf{R}^\\top)^{-1} = \\mathbf{Y} \\mathbf{R}^+,\n",
    "$$\n",
    "where $ \\mathbf{R}^+ $ is the Moore-Penrose pseudoinverse of $ \\mathbf{R} $."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
